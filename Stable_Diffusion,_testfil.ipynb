{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RainbowPowerr/ML-thesis/blob/main/Stable_Diffusion%2C_testfil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd-vX3cavOCt"
      },
      "source": [
        "# **Stable Diffusion** ðŸŽ¨ \n",
        "*...using `ðŸ§¨diffusers`*\n",
        "\n",
        "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It's trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM.\n",
        "See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information.\n",
        "\n",
        "This Colab notebook shows how to use Stable Diffusion with the ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers). \n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOlvQ1nQL7c"
      },
      "source": [
        "### Setup\n",
        "\n",
        "First, please make sure you are using a GPU runtime to run this notebook, so inference is much faster. If the following command fails, use the `Runtime` menu above and select `Change runtime type`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paJt_cx5QgVz"
      },
      "source": [
        "\n",
        "Next, you should install `diffusers==0.4.0` as well `scipy`, `ftfy` and `transformers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aIrgth7sqFML"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0891aa1022e84fb0b97e8ac6e592370b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fcyyt0daU4e"
      },
      "source": [
        "You also need to accept the model license before downloading or using the weights. In this post we'll use model version `v1-4`, so you'll need to  visit [its card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree. \n",
        "\n",
        "You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xSKWBKFPArKS"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8df086c412743f6ac21b0d0ca951ff6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=\"fp16\")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "all_images = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHaL8rNSH-95"
      },
      "source": [
        "# Nytt avsnitt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEErJFjlrSWS"
      },
      "outputs": [],
      "source": [
        "# \n",
        "generator = torch.Generator(\"cuda\").manual_seed(123)\n",
        "prompt = \"tiny red house surrounded by gold coin stacks, board game, 4k, hd\"\n",
        "image = pipe(prompt, num_inference_steps=50, guidance_scale = 8,  height= 480, width= 800).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "# or if you're in a google colab you can directly display it with \n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZdk190YweSt"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "all_images = []\n",
        "\n",
        "num_cols = 3\n",
        "num_rows = 4\n",
        "\n",
        "prompt = [\"/content/coinstack.jpg, straight view of a tiny red house next to a stack of golden coins, 4k, hd\"] * num_cols\n",
        "generator = torch.Generator(\"cuda\").manual_seed(984799)\n",
        "#negative_prompt = [\"text on boxes\"] * num_cols\n",
        "\n",
        "for i in range(num_rows):\n",
        "  images = pipe(prompt, num_inference_steps=50, guidance_scale = 8, generator=generator).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-FG0eNuQp4n"
      },
      "outputs": [],
      "source": [
        "generator = torch.Generator(\"cuda\").manual_seed(1234)\n",
        "prompt = \"/content/coinstack.jpg, straight view of a tiny red house next to a stack of golden coins, 4k, hd\"\n",
        "image = pipe(prompt, num_inference_steps=50, guidance_scale = 20, generator=generator, height=400 , width=720 ).images[0]\n",
        "\n",
        "image.save(\"/content/Test_images/Housing_market.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksuqjcSyeArJ"
      },
      "outputs": [],
      "source": [
        "generator = torch.Generator(\"cuda\").manual_seed(1234)\n",
        "prompt = \"/content/forest.jpg, a forest in Sweden by a lake, autumn, sun is shining,  hyperrealism, 4k, photo realistic, hd\"\n",
        "image = pipe(prompt, num_inference_steps=50, guidance_scale = 20, generator=generator, height=400 , width=720 ).images[0]\n",
        "\n",
        "image.save(\"/content/Test_images/forest.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmfC-Qz0ecy0"
      },
      "outputs": [],
      "source": [
        "generator = torch.Generator(\"cuda\").manual_seed(12345)\n",
        "prompt = \"/content/boxes.jpg, cardboard boxes and plants, on a table, in an office, folders, sideview, 4k, hd\"\n",
        "image = pipe(prompt, num_inference_steps=50, guidance_scale = 20, generator=generator, height=400 , width=720 ).images[0]\n",
        "\n",
        "image.save(\"/content/Test_images/bankruptcy.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voGoZGP2_sRU"
      },
      "outputs": [],
      "source": [
        "# Loop fÃ¶r att spara flera bilder i mappen Test_images\n",
        "\n",
        "num_images = 2\n",
        "\n",
        "prompt = [\"/content/coinstack.jpg, straight view of a tiny red house next to a stack of golden coins, 4k, hd\"]\n",
        "generator = torch.Generator(\"cuda\").manual_seed(984799)\n",
        "\n",
        "for i in range(num_images):\n",
        "  images = pipe(prompt, num_inference_steps=50, guidance_scale = 8, generator=generator).images\n",
        "  images[0].save(f'/content/Test_images/{i}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaAfNCoBe9sp"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/Test_images.zip /content/Test_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B4X9vMRUFla0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de3571ac782444f7961ddc788b920223",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/51 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#generator = torch.Generator(\"cuda\").manual_seed(1234)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/content/forest.jpg, a forest in Sweden by a lake, autumn, sun is shining,  hyperrealism, 4k, photo realistic, hd\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m image \u001b[39m=\u001b[39m pipe(prompt, num_inference_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, guidance_scale \u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m, height\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m , width\u001b[39m=\u001b[39;49m\u001b[39m720\u001b[39;49m )\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m image\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusion-jupyter/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusion-jupyter/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:320\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, output_type, return_dict, callback, callback_steps, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     noise_pred \u001b[39m=\u001b[39m noise_pred_uncond \u001b[39m+\u001b[39m guidance_scale \u001b[39m*\u001b[39m (noise_pred_text \u001b[39m-\u001b[39m noise_pred_uncond)\n\u001b[1;32m    319\u001b[0m \u001b[39m# compute the previous noisy sample x_t -> x_t-1\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m latents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscheduler\u001b[39m.\u001b[39;49mstep(noise_pred, t, latents, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_step_kwargs)\u001b[39m.\u001b[39mprev_sample\n\u001b[1;32m    322\u001b[0m \u001b[39m# call the callback, if provided\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m callback_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusion-jupyter/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:225\u001b[0m, in \u001b[0;36mPNDMScheduler.step\u001b[0;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_prk(model_output\u001b[39m=\u001b[39mmodel_output, timestep\u001b[39m=\u001b[39mtimestep, sample\u001b[39m=\u001b[39msample, return_dict\u001b[39m=\u001b[39mreturn_dict)\n\u001b[1;32m    224\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_plms(model_output\u001b[39m=\u001b[39;49mmodel_output, timestep\u001b[39m=\u001b[39;49mtimestep, sample\u001b[39m=\u001b[39;49msample, return_dict\u001b[39m=\u001b[39;49mreturn_dict)\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusion-jupyter/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:340\u001b[0m, in \u001b[0;36mPNDMScheduler.step_plms\u001b[0;34m(self, model_output, timestep, sample, return_dict)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     model_output \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m24\u001b[39m) \u001b[39m*\u001b[39m (\u001b[39m55\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m59\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m37\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mets[\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m])\n\u001b[0;32m--> 340\u001b[0m prev_sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_prev_sample(sample, timestep, prev_timestep, model_output)\n\u001b[1;32m    341\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusion-jupyter/lib/python3.10/site-packages/diffusers/schedulers/scheduling_pndm.py:376\u001b[0m, in \u001b[0;36mPNDMScheduler._get_prev_sample\u001b[0;34m(self, sample, timestep, prev_timestep, model_output)\u001b[0m\n\u001b[1;32m    374\u001b[0m alpha_prod_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas_cumprod[timestep]\n\u001b[1;32m    375\u001b[0m alpha_prod_t_prev \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphas_cumprod[prev_timestep] \u001b[39mif\u001b[39;00m prev_timestep \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_alpha_cumprod\n\u001b[0;32m--> 376\u001b[0m beta_prod_t \u001b[39m=\u001b[39m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m alpha_prod_t\n\u001b[1;32m    377\u001b[0m beta_prod_t_prev \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha_prod_t_prev\n\u001b[1;32m    379\u001b[0m \u001b[39m# corresponds to (Î±_(tâˆ’Î´) - Î±_t) divided by\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# denominator of x_t in formula (9) and plus 1\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# Note: (Î±_(tâˆ’Î´) - Î±_t) / (sqrt(Î±_t) * (sqrt(Î±_(tâˆ’Î´)) + sqr(Î±_t))) =\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# sqrt(Î±_(tâˆ’Î´)) / sqrt(Î±_t))\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/diffusion-jupyter/lib/python3.10/site-packages/torch/_tensor.py:37\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f, assigned\u001b[39m=\u001b[39massigned)\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[39m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m         \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     38\u001b[0m             \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#generator = torch.Generator(\"cuda\").manual_seed(1234)\n",
        "prompt = \"/content/forest.jpg, a forest in Sweden by a lake, autumn, sun is shining,  hyperrealism, 4k, photo realistic, hd\"\n",
        "image = pipe(prompt, num_inference_steps=50, guidance_scale = 20, height=400 , width=720 ).images[0]\n",
        "\n",
        "# Hello world\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O7D2ICAFzbC"
      },
      "outputs": [],
      "source": [
        "cinematic, colorful background, concept\n",
        "art, dramatic lighting, high detail, highly detailed, hyper realistic, intricate, intricate sharp details,\n",
        "octane render, smooth, studio lighting, trending on artstation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('diffusion-jupyter')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "9904bc9c0fe96b732ade1c75bb48c35357f0b2f3b0d99e8164bd4f164a7615d6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
